{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video_frames(frames, model, confidence_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Обработка кадров видео для детекции bounding boxes.\n",
    "    Если уверенность модели ниже заданного порога, используется предыдущий bounding box.\n",
    "\n",
    "    Параметры:\n",
    "    frames (list): Список кадров видео.\n",
    "    model (YOLOv3): Модель для предсказания bounding boxes.\n",
    "    confidence_threshold (float): Порог уверенности для использования bounding box.\n",
    "\n",
    "    Возвращает:\n",
    "    dict: Словарь с результатами для каждого кадра.\n",
    "    \"\"\"\n",
    "    last_boxes = {}  # Словарь для хранения последних уверенных bounding boxes по видео\n",
    "    results = {}  # Результаты для каждого кадра\n",
    "\n",
    "    for frame in frames:\n",
    "        video_id, frame_number = parse_frame_id(frame['id'])  # Разбор ID кадра на идентификатор видео и номер кадра\n",
    "        prediction = model(frame['image'])  # Предсказание модели для текущего кадра\n",
    "        confidence = prediction['confidence']  # Получение уверенности предсказания\n",
    "\n",
    "        if confidence < confidence_threshold:\n",
    "            # Использование bounding box из предыдущего кадра, если он есть\n",
    "            if video_id in last_boxes:\n",
    "                prediction['bbox'] = last_boxes[video_id]\n",
    "        else:\n",
    "            # Обновление последнего уверенного bounding box для текущего видео\n",
    "            last_boxes[video_id] = prediction['bbox']\n",
    "\n",
    "        results[frame['id']] = prediction  # Сохранение результата для текущего кадра\n",
    "\n",
    "    return results\n",
    "\n",
    "def parse_frame_id(frame_id):\n",
    "    \"\"\"\n",
    "    Разбор ID кадра на идентификатор видео и номер кадра.\n",
    "\n",
    "    Параметры:\n",
    "    frame_id (str): ID кадра.\n",
    "\n",
    "    Возвращает:\n",
    "    tuple: Идентификатор видео и номер кадра.\n",
    "    \"\"\"\n",
    "    parts = frame_id.split('_')\n",
    "    video_id = '_'.join(parts[:3])\n",
    "    frame_number = int(parts[3].split('.')[0])\n",
    "    return video_id, frame_number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.mse_loss = nn.MSELoss(reduction='sum')  # Для координат\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss(reduction='sum')  # Для уверенности\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        # Предсказанные уверенности и координаты\n",
    "        pred_confidences = predictions[:, 0]\n",
    "        pred_boxes = predictions[:, 1:]\n",
    "\n",
    "        # Целевые уверенности (все 1, так как все целевые боксы содержат объекты)\n",
    "        target_confidences = torch.ones_like(pred_confidences)\n",
    "\n",
    "        # Целевые координаты\n",
    "        target_boxes = targets\n",
    "\n",
    "        # Расчет потерь\n",
    "        confidence_loss = self.bce_loss(pred_confidences, target_confidences)\n",
    "        coordinate_loss = self.mse_loss(pred_boxes, target_boxes)\n",
    "\n",
    "        return confidence_loss + coordinate_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определение сверточного слоя с BatchNorm и LeakyReLU\n",
    "class ConvBNLeaky(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        super(ConvBNLeaky, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.leaky_relu(self.bn(self.conv(x)))\n",
    "\n",
    "# Определение остаточного блока\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = ConvBNLeaky(channels, channels // 2, 1, 1, 0)\n",
    "        self.conv2 = ConvBNLeaky(channels // 2, channels, 3, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.conv2(self.conv1(x))\n",
    "\n",
    "# Основная архитектура YOLOv3\n",
    "class YOLOv3(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(YOLOv3, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.layer1 = ConvBNLeaky(3, 32, 3, 1, 1)\n",
    "        self.layer2 = ConvBNLeaky(32, 64, 3, 2, 1)\n",
    "        self.residual_block1 = ResidualBlock(64)\n",
    "\n",
    "        # Дополнительные слои\n",
    "        self.layer3 = ConvBNLeaky(64, 128, 3, 2, 1)\n",
    "        self.residual_block2 = ResidualBlock(128)\n",
    "        self.layer4 = ConvBNLeaky(128, 256, 3, 2, 1)\n",
    "        self.residual_block3 = ResidualBlock(256)\n",
    "        self.layer5 = ConvBNLeaky(256, 512, 3, 2, 1)\n",
    "        self.residual_block4 = ResidualBlock(512)\n",
    "        self.layer6 = ConvBNLeaky(512, 1024, 3, 2, 1)\n",
    "        self.residual_block5 = ResidualBlock(1024)\n",
    "\n",
    "        # Слои обнаружения\n",
    "        self.detection1 = nn.Conv2d(1024, 5 * num_classes, 1)\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.final_conv = nn.Conv2d(1024, 5, 1)  # Изменил 4 на 5, чтобы включить уверенность\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.residual_block1(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.residual_block2(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.residual_block3(x)\n",
    "        x = self.layer5(x)\n",
    "        x = self.residual_block4(x)\n",
    "        x = self.layer6(x)\n",
    "        x = self.residual_block5(x)\n",
    "\n",
    "        x = self.global_avg_pool(x)  # Применение глобального среднего пулинга\n",
    "        detection = self.final_conv(x)  # Применение конечного сверточного слоя\n",
    "        detection = detection.view(x.size(0), -1)  # Изменение формы тензора\n",
    "\n",
    "        # Применяем сигмоиду к первому элементу каждого предсказания для уверенности\n",
    "        # Предполагаем, что detection имеет форму [N, 5], где N - количество предсказаний\n",
    "        detection[:, 0] = torch.sigmoid(detection[:, 0])\n",
    "        return detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YourDataset(Dataset):\n",
    "    def __init__(self, img_dir, ann_dir, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.ann_dir = ann_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Получение списка имен файлов изображений\n",
    "        self.img_names = [img_name for img_name in os.listdir(img_dir) if img_name.endswith('.jpg')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.img_names[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        ann_path = os.path.join(self.ann_dir, img_name.replace('.jpg', '.txt'))\n",
    "\n",
    "        # Загрузка изображения\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        # Загрузка аннотаций\n",
    "        annotations = self.load_annotations(ann_path)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, annotations\n",
    "\n",
    "    @staticmethod\n",
    "    def load_annotations(ann_path):\n",
    "        annotations = []\n",
    "        with open(ann_path, 'r') as file:\n",
    "            for line in file:\n",
    "                _, x_center, y_center, width, height = map(float, line.split())\n",
    "                annotations.append([x_center, y_center, width, height])\n",
    "        return torch.tensor(annotations).squeeze() # оно по дефолту делает тензор размера [1, 4] а надо [4] потому что у кажного объекта ровно 1 аннотация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Теперь пути к данным должны указывать на распакованные каталоги\n",
    "img_dir = '/kaggle/input/brikerdataset/dataset0904/train_images'  # Пример пути к изображениям для обучения\n",
    "ann_dir = '/kaggle/input/brikerdataset/dataset0904/train_annotations'  # Пример пути к аннотациям для обучения\n",
    "\n",
    "test_img_dir = '/kaggle/input/brikerdataset/dataset0904/test_images'  # Пример пути к изображениям для тестирования\n",
    "test_ann_dir = '/kaggle/input/brikerdataset/dataset0904/test_annotations'  # Пример пути к аннотациям для тестирования\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls '/kaggle/input/brikerdataset/dataset0904/test_images' | head -n 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание модели\n",
    "model = YOLOv3(num_classes=1)\n",
    "\n",
    "# Проверка доступности CUDA\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "# Создание экземпляра функции потерь и оптимизатора\n",
    "criterion = CustomLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = YourDataset(img_dir, ann_dir, transform=transform)\n",
    "test_dataset = YourDataset(test_img_dir, test_ann_dir, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=0, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.ops import box_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_yolo_to_corners(bboxes, img_width, img_height):\n",
    "    # Конвертация из формата YOLO (x_center, y_center, width, height) в формат координат углов (x_min, y_min, x_max, y_max)\n",
    "    converted_bboxes = []\n",
    "    for bbox in bboxes:\n",
    "        x_center, y_center, width, height = bbox[-4:]\n",
    "        x_min = (x_center - width / 2) * img_width\n",
    "        y_min = (y_center - height / 2) * img_height\n",
    "        x_max = (x_center + width / 2) * img_width\n",
    "        y_max = (y_center + height / 2) * img_height\n",
    "        converted_bboxes.append([x_min, y_min, x_max, y_max])\n",
    "    return torch.tensor(converted_bboxes, dtype=torch.float32)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confidence_and_bbox(outputs):\n",
    "    \"\"\"\n",
    "    Разделяет выходные данные модели на уверенности и координаты bounding boxes.\n",
    "\n",
    "    Параметры:\n",
    "        outputs (torch.Tensor): Тензор с выходными данными модели размерности [batch_size, 5],\n",
    "                                где каждая строка содержит [confidence, x, y, width, height].\n",
    "\n",
    "    Возвращает:\n",
    "        Tuple[torch.Tensor, torch.Tensor]:\n",
    "        - Вектор уверенностей размерности [batch_size],\n",
    "        - Матрицу координат bounding boxes размерности [batch_size, 4].\n",
    "    \"\"\"\n",
    "    # Предполагаем, что outputs уже находится на CPU, если необходимо\n",
    "    confidences = outputs[:, 0]  # Все строки, первый столбец\n",
    "    bboxes = outputs[:, 1:]  # Все строки, со второго столбца до последнего\n",
    "\n",
    "    return confidences, bboxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_threshold = 0.50  # Порог уверенности для демонстрации\n",
    "last_confident_bbox = None  # Для хранения последнего уверенного bounding box\n",
    "\n",
    "train_losses, val_losses, train_ious, val_ious = [], [], [], []\n",
    "\n",
    "num_epochs = 10\n",
    "image_width, image_height = 224, 224  # Примерные размеры изображения, измените на ваши\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_mse, train_iou_accum = 0, 0\n",
    "    print(f'Epoch: {epoch}')\n",
    "    for batch_idx, (images, targets) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            images, targets = images.cuda(), targets.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "        train_mse += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        for target in targets:\n",
    "            target = target.squeeze() # надо чтобы тензор был размера [4] а сейчас он  [1, 4]\n",
    "\n",
    "        outputs = outputs.detach().cpu()\n",
    "        targets = targets.detach().cpu()\n",
    "\n",
    "        # Преобразование формата и расчет IoU для текущего пакета\n",
    "        predicted_corners = convert_yolo_to_corners(outputs, image_width, image_height)\n",
    "        target_corners = convert_yolo_to_corners(targets, image_width, image_height)\n",
    "        iou_scores = box_iou(predicted_corners, target_corners)\n",
    "        train_iou_accum += iou_scores.diag().mean().item()\n",
    "\n",
    "    average_train_mse = train_mse / len(train_loader)\n",
    "    average_train_iou = train_iou_accum / len(train_loader)\n",
    "    train_losses.append(average_train_mse)\n",
    "    train_ious.append(average_train_iou)\n",
    "\n",
    "    model.eval()\n",
    "    val_mse, val_iou_accum = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in test_loader:\n",
    "            if torch.cuda.is_available():\n",
    "                images, targets = images.cuda(), targets.cuda()\n",
    "\n",
    "            outputs = model(images)\n",
    "            # Преобразуем выводы модели в удобный формат (предполагается реализация функции)\n",
    "            # Эта часть кода должна быть адаптирована под вашу модель и ее вывод\n",
    "            confidences, bboxes = get_confidence_and_bbox(outputs)\n",
    "\n",
    "            for i, confidence in enumerate(confidences):\n",
    "                if confidence < confidence_threshold and last_confident_bbox is not None:\n",
    "                    # Если уверенность ниже порога и есть сохраненный bbox, используем его\n",
    "                    bboxes[i] = last_confident_bbox\n",
    "                else:\n",
    "                    # Иначе обновляем последний надежный bbox\n",
    "                    last_confident_bbox = bboxes[i]\n",
    "\n",
    "            # Теперь bboxes содержит текущие или последние надежные предсказания\n",
    "            # Продолжаем с расчетом потерь и IoU используя обновленные bboxes\n",
    "            loss = criterion(outputs, targets) # было bboxes, стало outputs\n",
    "            val_mse += loss.item()\n",
    "\n",
    "            # Предполагается, что для IoU требуются координаты углов, здесь используем обновленные bboxes\n",
    "            predicted_corners = convert_yolo_to_corners(bboxes, image_width, image_height)\n",
    "            target_corners = convert_yolo_to_corners(targets, image_width, image_height)\n",
    "            iou_scores = box_iou(predicted_corners, target_corners)\n",
    "            val_iou_accum += iou_scores.diag().mean().item()\n",
    "\n",
    "    average_val_loss = val_mse / len(test_loader)\n",
    "    average_val_iou = val_iou_accum / len(test_loader)\n",
    "    val_losses.append(average_val_loss)\n",
    "    val_ious.append(average_val_iou)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Train Loss: {average_train_mse}, Train IoU: {average_train_iou}, Val Loss: {average_val_loss}, Val IoU: {average_val_iou}\")\n",
    "\n",
    "# Визуализация потерь и IoU\n",
    "epochs = range(1, num_epochs + 1)\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_losses, label='Training Loss')\n",
    "plt.plot(epochs, val_losses, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_ious, label='Training IoU')\n",
    "plt.plot(epochs, val_ious, label='Validation IoU')\n",
    "plt.title('Training and Validation IoU')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('IoU')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Epoch 1, Train Loss: 85.71187585975692, Train IoU: 0.4877332774663551, Val Loss: 46.21363431757147, Val IoU: 0.46543790196830576\n",
    "\n",
    "Epoch: 1\n",
    "\n",
    "Epoch 2, Train Loss: 43.73765919780173, Train IoU: 0.566342073922966, Val Loss: 45.1648154258728, Val IoU: 0.5417444814335216\n",
    "\n",
    "Epoch: 2\n",
    "\n",
    "Epoch 3, Train Loss: 43.16679687388459, Train IoU: 0.5967341888892023, Val Loss: 44.5598879293962, Val IoU: 0.538937889716842\n",
    "\n",
    "Epoch: 3\n",
    "\n",
    "Epoch 4, Train Loss: 42.48168433339972, Train IoU: 0.6337331101211191, Val Loss: 43.79320101304488, Val IoU: 0.590618992393667\n",
    "\n",
    "Epoch: 4\n",
    "\n",
    "Epoch 5, Train Loss: 42.191190468637565, Train IoU: 0.6529127280963095, Val Loss: 43.472356362776324, Val IoU: 0.5706874023784291\n",
    "\n",
    "Epoch: 5\n",
    "\n",
    "Epoch 6, Train Loss: 41.97090085905198, Train IoU: 0.6673855291821106, Val Loss: 45.67795380679044, Val IoU: 0.38146351785822347\n",
    "\n",
    "Epoch: 6\n",
    "\n",
    "Epoch 7, Train Loss: 41.62586819498163, Train IoU: 0.7002999914145609, Val Loss: 42.955378012223676, Val IoU: 0.6122458814219995\n",
    "\n",
    "Epoch: 7\n",
    "\n",
    "Epoch 8, Train Loss: 41.56665186296429, Train IoU: 0.6955540673426021, Val Loss: 43.48175668716431, Val IoU: 0.5917525684291666\n",
    "\n",
    "Epoch: 8\n",
    "\n",
    "Epoch 9, Train Loss: 41.335165843629, Train IoU: 0.7191016091937907, Val Loss: 42.64876096898859, Val IoU: 0.6044311360879377\n",
    "\n",
    "Epoch: 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "\n",
    "def draw_bbox(image, bbox, color='red'):\n",
    "    \"\"\"\n",
    "    Рисует ограничивающую рамку на изображении.\n",
    "\n",
    "    Аргументы:\n",
    "    image - изображение в формате PIL или путь к изображению.\n",
    "    bbox - координаты рамки в формате [x_center, y_center, width, height].\n",
    "    color - цвет рамки.\n",
    "    \"\"\"\n",
    "    if isinstance(image, str):\n",
    "        image = Image.open(image)\n",
    "\n",
    "    # Преобразование координат YOLO в абсолютные координаты\n",
    "    img_width, img_height = image.size\n",
    "    x_center, y_center, width, height = bbox\n",
    "    x_min = (x_center - width / 2) * img_width\n",
    "    y_min = (y_center - height / 2) * img_height\n",
    "    abs_width = width * img_width\n",
    "    abs_height = height * img_height\n",
    "\n",
    "    # Создание фигуры и осей\n",
    "    fig,ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "\n",
    "    # Создание прямоугольника вокруг объекта\n",
    "    rect = patches.Rectangle((x_min, y_min), abs_width, abs_height, linewidth=1, edgecolor=color, facecolor='none')\n",
    "\n",
    "    # Добавление прямоугольника на график\n",
    "    ax.add_patch(rect)\n",
    "    plt.show()\n",
    "\n",
    "# Пример использования\n",
    "# Загрузите изображение используя PIL: img = Image.open('path_to_your_image.jpg')\n",
    "# Предполагается, что bbox - это предсказание модели в формате YOLO: [x_center, y_center, width, height]\n",
    "# draw_bbox(img, [0.5, 0.5, 0.2, 0.3], 'red')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "# Загрузка изображения\n",
    "image_path = '/kaggle/input/brikerdataset/dataset0904/test_images/grc_passport_82_000181.jpg'\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Предобработка изображения\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "image_transformed = transform(image).unsqueeze(0)  # Добавляем батч-размерность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "# Предполагается, что модель уже загружена и готова к использованию\n",
    "model.eval()  # Переключение в режим оценки\n",
    "with torch.no_grad():  # Выключение вычисления градиентов для ускорения\n",
    "    prediction = model(image_transformed.to(device))  # device может быть 'cuda' или 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример предсказания: [0.5, 0.5, 0.2, 0.3], предполагается что это numpy массив или список\n",
    "bbox = prediction[0].cpu().numpy()  # Преобразование в numpy массив, если это ещё не сделано\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вызов функции для отрисовки рамки\n",
    "draw_bbox(image, bbox, 'red')  # Используйте реальные размеры вашего изображения\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import box_iou\n",
    "\n",
    "# Предсказанные координаты (замените эти значения на ваши реальные предсказания)\n",
    "predicted_bbox = torch.tensor(bbox)\n",
    "# Истинные координаты\n",
    "true_bbox = torch.tensor([0.5016203703703703, 0.5298177083333333, 0.9041666666666667, 0.3190104166666667])\n",
    "\n",
    "# Преобразование координат из YOLO в абсолютные координаты рамки\n",
    "print(predicted_bbox)\n",
    "predicted_corners = convert_yolo_to_corners([predicted_bbox], 224, 224)\n",
    "true_corners = convert_yolo_to_corners([true_bbox], 224, 224)\n",
    "\n",
    "# Преобразование в тензоры PyTorch\n",
    "predicted_corners_tensor = torch.tensor(predicted_corners, dtype=torch.float)\n",
    "true_corners_tensor = torch.tensor(true_corners, dtype=torch.float)\n",
    "\n",
    "# Расчет IoU\n",
    "iou_score = box_iou(predicted_corners_tensor, true_corners_tensor)\n",
    "\n",
    "iou_score.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pth')\n",
    "from google.colab import files\n",
    "\n",
    "files.download('model.pth')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4785687,
     "sourceId": 8103326,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30683,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
