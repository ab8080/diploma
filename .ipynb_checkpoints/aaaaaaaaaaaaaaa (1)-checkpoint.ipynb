{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8103326,"sourceType":"datasetVersion","datasetId":4785687}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.utils.data import Dataset\nimport os\nfrom PIL import Image","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-07T21:24:03.805621Z","iopub.execute_input":"2024-05-07T21:24:03.805991Z","iopub.status.idle":"2024-05-07T21:24:07.462485Z","shell.execute_reply.started":"2024-05-07T21:24:03.805959Z","shell.execute_reply":"2024-05-07T21:24:07.461556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomLoss(nn.Module):\n    def __init__(self):\n        super(CustomLoss, self).__init__()\n        self.mse_loss = nn.MSELoss(reduction='sum')  # Для координат\n        self.bce_loss = nn.BCEWithLogitsLoss(reduction='sum')  # Для уверенности\n\n    def forward(self, predictions, targets):\n        # Плоское представление для упрощения операций\n        # predictions shape: [batch_size, seq_len, 5] \n        # targets shape: [batch_size, seq_len, 1, 5]\n        predictions = predictions.view(-1, 5)  # Сглаживаем батч и временные шаги\n        targets = targets.view(-1, 5)  # Сглаживаем батч, временные шаги и учитываем, что targets уже содержат 5 значений\n\n        # Предсказанные уверенности и координаты\n        pred_confidences = predictions[:, 0]  # Первый столбец - уверенность\n        pred_boxes = predictions[:, 1:]  # Остальные столбцы - координаты\n\n        # Целевые уверенности (последний столбец в targets)\n        target_confidences = targets[:, 0]  # Используем реальные значения уверенности из targets\n\n        # Целевые координаты\n        target_boxes = targets[:, 1:]  # Пропускаем столбец уверенности в targets\n\n        # Расчет потерь\n        confidence_loss = self.bce_loss(pred_confidences, target_confidences)\n        coordinate_loss = self.mse_loss(pred_boxes, target_boxes)  # Совпадение размеров уже гарантировано\n\n        # Возвращаем сумму потерь\n        return confidence_loss + coordinate_loss\n","metadata":{"execution":{"iopub.status.busy":"2024-05-07T21:24:54.667280Z","iopub.execute_input":"2024-05-07T21:24:54.668081Z","iopub.status.idle":"2024-05-07T21:24:54.676370Z","shell.execute_reply.started":"2024-05-07T21:24:54.668048Z","shell.execute_reply":"2024-05-07T21:24:54.675320Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Определение Symmetrical ReLU (SymReLU)\nclass SymReLU(nn.Module):\n    def __init__(self, alpha=1.0):\n        super(SymReLU, self).__init__()\n        self.alpha = alpha\n    \n    def forward(self, x):\n        return torch.max(x, torch.zeros_like(x)) - self.alpha * torch.min(x, torch.zeros_like(x))\n\n# Определение слоя свертки с SymReLU\nclass ConvSymRelu(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, groups=1):\n        super(ConvSymRelu, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, groups=groups, bias=False)\n        self.symrelu = SymReLU(alpha=1.0)\n    \n    def forward(self, x):\n        return self.symrelu(self.conv(x))\n    \nclass NewYOLOv3(nn.Module):\n    def __init__(self, num_classes, lstm_hidden_size=512, lstm_layers=1, seq_len=8):\n        super(NewYOLOv3, self).__init__()\n        # Определение сверточных слоев\n        self.conv0_yolo = ConvSymRelu(3, 12, (5, 5), (1, 1), (2, 2))\n        self.maxpool0 = nn.MaxPool2d((2, 2), stride=(2, 2))\n        self.conv1 = ConvSymRelu(12, 16, (5, 5), (2, 2), (1, 1))\n        self.conv2 = ConvSymRelu(16, 16, (3, 3), (2, 2), (1, 1))\n        self.conv3 = ConvSymRelu(16, 16, (3, 3), (1, 1), (1, 1))\n        self.pool4 = nn.MaxPool2d((2, 2), stride=(2, 2))\n        self.conv5 = ConvSymRelu(16, 24, (3, 3), (1, 1), (1, 1))\n        self.conv6 = ConvSymRelu(24, 48, (3, 3), (2, 2), (1, 1))\n        self.conv7 = ConvSymRelu(48, 48, (3, 3), (1, 1), (1, 1))\n        self.conv8 = ConvSymRelu(48, 48, (3, 3), (1, 1), (1, 1))\n        self.conv9 = ConvSymRelu(48, 48, (3, 3), (1, 1), (1, 1))\n        self.conv11 = ConvSymRelu(48, 64, (3, 3), (1, 1), (1, 1))\n        self.conv13 = ConvSymRelu(64, 64, (7, 7), (7, 7), (0, 0))  # Уменьшаем до 1x1\n        self.conv14 = ConvSymRelu(64, 4, (1, 1), (1, 1), (0, 0))  # Меняем количество каналов, сохраняем 1x1\n        self.conv14_1 = ConvSymRelu(4, 4, (3, 3), (1, 1), (1, 1))\n        self.conv14_2 = ConvSymRelu(4, 4, (3, 3), (1, 1), (1, 1))\n        self.upsample0 = nn.Upsample(scale_factor=(1, 1), mode='nearest')\n        self.conv15 = ConvSymRelu(4, 96, (3, 3), (1, 1), (1, 1))\n        self.prefinal = ConvSymRelu(96, 512, (1, 1), (1, 1), (0, 0))\n        self.final = nn.Conv2d(512, 512, (1, 1))\n        \n        # Определение GRU слоя\n        self.gru = nn.GRU(input_size=512, hidden_size=lstm_hidden_size, num_layers=lstm_layers, batch_first=True)\n        self.fc = nn.Linear(lstm_hidden_size, 5)\n        self.seq_len = seq_len\n\n    def forward(self, x):\n\n        batch_size, seq_len, C, H, W = x.size()\n        # Сверточная часть\n        x = x.view(batch_size * seq_len, C, H, W)  \n        x = self.conv0_yolo(x)\n        x = self.maxpool0(x)\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.pool4(x)\n        x = self.conv5(x)\n        x = self.conv6(x)\n        x = self.conv7(x)\n        x = self.conv8(x)\n        x = self.conv9(x)\n        x = self.conv11(x)\n        x = self.conv13(x)\n        x = self.conv14(x)\n        x = self.conv14_1(x)\n        x = self.conv14_2(x)\n        x = self.upsample0(x)\n        x = self.conv15(x)\n        x = self.prefinal(x)\n        x = self.final(x)\n        x = x.view(batch_size, seq_len, -1)  # Подготовка входа для GRU\n\n        # Рекуррентная часть\n        hidden = None  # GRU сам инициализирует скрытое состояние, если не задано\n        x, hidden = self.gru(x) # возможно надо убрать hidden\n        # Применение последнего слоя\n        x = self.fc(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-05-07T21:24:57.567229Z","iopub.execute_input":"2024-05-07T21:24:57.568038Z","iopub.status.idle":"2024-05-07T21:24:57.593423Z","shell.execute_reply.started":"2024-05-07T21:24:57.568006Z","shell.execute_reply":"2024-05-07T21:24:57.592422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torchvision import transforms\nimport random\nimport numpy as np\nfrom PIL import Image, ImageFilter\nimport torchvision.transforms.functional as TF\n\nclass YourDataset(torch.utils.data.Dataset):\n    def __init__(self, img_dir, ann_dir, transform=None):\n        self.img_dir = img_dir\n        self.ann_dir = ann_dir\n        self.transform = transform\n\n        self.img_names = [img_name for img_name in os.listdir(img_dir) if img_name.endswith('.jpg')]\n        self.img_names.sort(key=lambda x: (x.split('_')[0], int(x.split('_')[2]), int(x.split('_')[3].split('.')[0])))\n\n        self.grouped_images = {}\n        for img_name in self.img_names:\n            key = '_'.join(img_name.split('_')[:3])\n            if key not in self.grouped_images:\n                self.grouped_images[key] = []\n            self.grouped_images[key].append(img_name)\n\n        self.final_img_names = []\n        for key, imgs in self.grouped_images.items():\n            if len(imgs) >= 8:\n                self.final_img_names.extend(imgs[:len(imgs) - (len(imgs) % 8)])\n\n    def __len__(self):\n        return len(self.final_img_names) // 8\n\n    def __getitem__(self, idx):\n        images = []\n        annotations = []\n        start_index = idx * 8\n        noise = self.random_noise()\n        blur_filter = self.random_blur()\n        angle = random.uniform(-30, 30)  # случайный угол вращения от -30 до 30 градусов\n\n        for i in range(8):\n            img_name = self.final_img_names[start_index + i]\n            img_path = os.path.join(self.img_dir, img_name)\n            ann_path = os.path.join(self.ann_dir, img_name.replace('.jpg', '.txt'))\n\n            image = Image.open(img_path).convert('RGB')\n            image = image.filter(blur_filter)  # применение размытия\n            image = TF.rotate(image, angle)  # применение вращения\n\n            if self.transform:\n                image = self.transform(image)\n            print(\"angle: \", angle)\n            print(\"ann: \", self.load_annotations(ann_path))\n            annotations_rotated = self.rotate_annotations(self.load_annotations(ann_path), angle, image.size)\n            print(\"rotated ann: \", annotations_rotated)\n            print(\"-----------------------\")\n            # Преобразование в тензор и добавление шума\n            image_tensor = image + noise\n            image_tensor = torch.clamp(image_tensor, 0, 1)\n\n            images.append(image_tensor)\n            annotations.append(torch.tensor(annotations_rotated))\n\n        images = torch.stack(images)\n        annotations = torch.stack(annotations)\n        return images, annotations\n\n    @staticmethod\n    def load_annotations(ann_path):\n        annotations = []\n        with open(ann_path, 'r') as file:\n            for line in file:\n                _, x_center, y_center, width, height = map(float, line.split())\n                annotations.append([1, x_center, y_center, width, height])\n        return annotations\n\n    @staticmethod\n    def random_noise():\n        std_dev = torch.empty(1).uniform_(0.01, 0.05)\n        noise = torch.randn((3, 224, 224)) * std_dev\n        return noise\n\n    @staticmethod\n    def random_blur():\n        radius = random.uniform(0.2, 2.0)\n        return ImageFilter.GaussianBlur(radius)\n\n    def rotate_annotations(self, annotations, angle, image_size):\n        angle_rad = math.radians(angle)  # Угол в радианах\n        cos_val, sin_val = math.cos(angle_rad), math.sin(angle_rad)\n        w, h = 224,224\n        new_annotations = []\n\n        for _, x_center, y_center, width, height in annotations:\n            # Перевод координат центра bbox в систему координат с центром в центре изображения\n            x_center = x_center * w - w / 2\n            y_center = y_center * h - h / 2\n\n            # Поворот координат центра bbox\n            x_new = x_center * cos_val - y_center * sin_val\n            y_new = x_center * sin_val + y_center * cos_val\n\n            # Возвращение координат в исходную систему координат\n            x_new = x_new + w / 2\n            y_new = y_new + h / 2\n\n            # Возврат координат к относительным значениям\n            x_new = x_new / w\n            y_new = y_new / h\n\n            new_annotations.append([1, x_new, y_new, width, height])\n\n        return new_annotations\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-07T22:44:00.357722Z","iopub.execute_input":"2024-05-07T22:44:00.358631Z","iopub.status.idle":"2024-05-07T22:44:00.382583Z","shell.execute_reply.started":"2024-05-07T22:44:00.358597Z","shell.execute_reply":"2024-05-07T22:44:00.381511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision import transforms\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\nfrom torchvision.ops import box_iou\n","metadata":{"execution":{"iopub.status.busy":"2024-05-07T21:25:08.685791Z","iopub.execute_input":"2024-05-07T21:25:08.686748Z","iopub.status.idle":"2024-05-07T21:25:08.690968Z","shell.execute_reply.started":"2024-05-07T21:25:08.686714Z","shell.execute_reply":"2024-05-07T21:25:08.690024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Теперь пути к данным должны указывать на распакованные каталоги\nimg_dir = '/kaggle/input/brikerdataset/dataset0904/train_images'  # Пример пути к изображениям для обучения\nann_dir = '/kaggle/input/brikerdataset/dataset0904/train_annotations'  # Пример пути к аннотациям для обучения\n\ntest_img_dir = '/kaggle/input/brikerdataset/dataset0904/test_images'  # Пример пути к изображениям для тестирования\ntest_ann_dir = '/kaggle/input/brikerdataset/dataset0904/test_annotations'  # Пример пути к аннотациям для тестирования","metadata":{"execution":{"iopub.status.busy":"2024-05-07T21:26:36.180583Z","iopub.execute_input":"2024-05-07T21:26:36.181521Z","iopub.status.idle":"2024-05-07T21:26:36.187002Z","shell.execute_reply.started":"2024-05-07T21:26:36.181486Z","shell.execute_reply":"2024-05-07T21:26:36.186256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size=16\nseq_len=8","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:18:34.662831Z","iopub.execute_input":"2024-05-06T19:18:34.663241Z","iopub.status.idle":"2024-05-06T19:18:34.668737Z","shell.execute_reply.started":"2024-05-06T19:18:34.663208Z","shell.execute_reply":"2024-05-06T19:18:34.667704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nimport matplotlib.patches as patches\nimport matplotlib.pyplot as plt\n\n# Предположим, что YourDataset уже определен как в вашем предыдущем коде\n\n\n# Определение трансформаций для изображений\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor()\n])\n\n# Создание экземпляра датасета\ndataset = YourDataset(img_dir, ann_dir, transform=transform)\n\n# DataLoader для загрузки данных\nloader = DataLoader(dataset, batch_size=1, shuffle=True)\n\ndef plot_images(batch_images, batch_annotations):\n    \"\"\" Функция для отрисовки изображений с bounding boxes. \"\"\"\n    fig, axs = plt.subplots(nrows=2, ncols=4, figsize=(12, 6))  # 8 изображений на батч\n    axs = axs.flatten()\n    \n    for i in range(8):  # Перебираем каждое изображение в \"мини-батче\"\n        img = batch_images[0][i].permute(1, 2, 0)  # Перестановка каналов для отображения\n        ax = axs[i]\n        ax.imshow(img.numpy())\n        \n        img_height, img_width = img.shape[0], img.shape[1]\n        ann = batch_annotations[0][i]  # Аннотации для текущего изображения\n        \n        # Отображаем каждый bounding box\n        for _, x_center, y_center, width, height in ann:\n            # Пересчет координат и размеров для масштабирования к размерам изображения\n            x = (x_center - width / 2) * img_width\n            y = (y_center - height / 2) * img_height\n            width = width * img_width\n            height = height * img_height\n            \n            rect = patches.Rectangle((x, y), width, height, linewidth=1, edgecolor='r', facecolor='none')\n            ax.add_patch(rect)\n        ax.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Предполагается, что DataLoader уже создан и настроен\n\nfor batch_images, batch_annotations in loader:\n    plot_images(batch_images, batch_annotations)  # Обработка всего мини-батча\n\n    break  # Пример с одним батчем для демонстрации\n","metadata":{"execution":{"iopub.status.busy":"2024-05-07T22:44:03.532099Z","iopub.execute_input":"2024-05-07T22:44:03.532477Z","iopub.status.idle":"2024-05-07T22:44:08.741611Z","shell.execute_reply.started":"2024-05-07T22:44:03.532448Z","shell.execute_reply":"2024-05-07T22:44:08.740596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_images[0].shape","metadata":{"execution":{"iopub.status.busy":"2024-05-07T21:58:59.194649Z","iopub.execute_input":"2024-05-07T21:58:59.195086Z","iopub.status.idle":"2024-05-07T21:58:59.201504Z","shell.execute_reply.started":"2024-05-07T21:58:59.195052Z","shell.execute_reply":"2024-05-07T21:58:59.200576Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Создание экземпляра новой модели\nmodel = NewYOLOv3(num_classes=1)  # Обратите внимание на количество классов\n\n# Проверка доступности CUDA\nif torch.cuda.is_available():\n    model = model.cuda()\n\n# Создание экземпляра функции потерь и оптимизатора\ncriterion = CustomLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# Подготовка датасетов, загрузчиков и трансформаций, если они нужны\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor()\n])\n\ntrain_dataset = YourDataset(img_dir, ann_dir, transform=transform)\ntest_dataset = YourDataset(test_img_dir, test_ann_dir, transform=transform)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=1, pin_memory=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=1, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:18:39.269613Z","iopub.execute_input":"2024-05-06T19:18:39.270363Z","iopub.status.idle":"2024-05-06T19:18:39.550439Z","shell.execute_reply.started":"2024-05-06T19:18:39.270329Z","shell.execute_reply":"2024-05-06T19:18:39.549424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_yolo_to_corners(bboxes, img_width, img_height):\n    # Предполагаем, что размеры bboxes: [batch_size, seq_len, 5]\n    # Первый элемент - confidence, игнорируем его при конвертации координат.\n    batch_size, seq_len, _ = bboxes.shape\n    converted_bboxes = []\n    for batch in range(batch_size):\n        batch_converted_bboxes = []\n        for seq in range(seq_len):\n            # Берем только координаты x_center, y_center, width, height\n            x_center, y_center, width, height = bboxes[batch, seq, 1:5]\n            x_min = (x_center - width / 2) * img_width\n            y_min = (y_center - height / 2) * img_height\n            x_max = (x_center + width / 2) * img_width\n            y_max = (y_center + height / 2) * img_height\n            batch_converted_bboxes.append([x_min, y_min, x_max, y_max])\n        converted_bboxes.append(batch_converted_bboxes)\n    return torch.tensor(converted_bboxes, dtype=torch.float32)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T18:47:28.276462Z","iopub.execute_input":"2024-05-06T18:47:28.276779Z","iopub.status.idle":"2024-05-06T18:47:28.283981Z","shell.execute_reply.started":"2024-05-06T18:47:28.276756Z","shell.execute_reply":"2024-05-06T18:47:28.282973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_confidence_and_bbox(outputs):\n    \"\"\"\n    Разделяет выходные данные модели на уверенности и координаты bounding boxes.\n\n    Параметры:\n        outputs (torch.Tensor): Тензор с выходными данными модели размерности [batch_size, 5],\n                                где каждая строка содержит [confidence, x, y, width, height].\n\n    Возвращает:\n        Tuple[torch.Tensor, torch.Tensor]:\n        - Вектор уверенностей размерности [batch_size],\n        - Матрицу координат bounding boxes размерности [batch_size, 4].\n    \"\"\"\n    # Предполагаем, что outputs уже находится на CPU, если необходимо\n    confidences = outputs[:, 0]  # Все строки, первый столбец\n    bboxes = outputs[:, 1:]  # Все строки, со второго столбца до последнего\n\n    return confidences, bboxes","metadata":{"execution":{"iopub.status.busy":"2024-05-06T18:47:28.284894Z","iopub.execute_input":"2024-05-06T18:47:28.285148Z","iopub.status.idle":"2024-05-06T18:47:28.296587Z","shell.execute_reply.started":"2024-05-06T18:47:28.285126Z","shell.execute_reply":"2024-05-06T18:47:28.295683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_iou_batch(predicted_corners, target_corners):\n    # Подготовка тензоров для функции box_iou\n    # predicted_corners и target_corners должны иметь форму [batch_size, seq_len, 4]\n    batch_size, seq_len, _ = predicted_corners.shape\n    ious = torch.zeros(batch_size, seq_len)  # Тензор для хранения значений IoU\n\n    # Вычисляем IoU для каждой пары предсказанных и целевых углов в батче\n    for batch in range(batch_size):\n        for seq in range(seq_len):\n            # Переформатирование тензоров для соблюдения ожидаемой размерности в box_iou\n            pred_box = predicted_corners[batch, seq].unsqueeze(0)\n            target_box = target_corners[batch, seq].unsqueeze(0)\n            iou = box_iou(pred_box, target_box)\n            ious[batch, seq] = iou  # Запись результата IoU в тензор\n\n    return ious","metadata":{"execution":{"iopub.status.busy":"2024-05-06T18:47:28.297623Z","iopub.execute_input":"2024-05-06T18:47:28.297885Z","iopub.status.idle":"2024-05-06T18:47:28.309793Z","shell.execute_reply.started":"2024-05-06T18:47:28.297862Z","shell.execute_reply":"2024-05-06T18:47:28.308972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_checkpoint(state, filename=\"model_checkpoint.pth\"):\n    \"\"\"Сохранение текущего состояния модели\"\"\"\n    torch.save(state, filename)\n\ndef plot_metrics(train_losses, val_losses, train_ious, val_ious, epoch):\n    \"\"\"Отрисовка графиков потерь и IoU\"\"\"\n    epochs = range(1, epoch + 2)\n    plt.figure(figsize=(12, 6))\n\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, train_losses, label='Training Loss')\n    plt.plot(epochs, val_losses, label='Validation Loss')\n    plt.title('Training and Validation Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, train_ious, label='Training IoU')\n    plt.plot(epochs, val_ious, label='Validation IoU')\n    plt.title('Training and Validation IoU')\n    plt.xlabel('Epoch')\n    plt.ylabel('IoU')\n    plt.legend()\n\n    plt.tight_layout()\n    plt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport time\nconfidence_threshold = 0.50  # Порог уверенности для демонстрации\nlast_confident_bbox = None  # Для хранения последнего уверенного bounding box\n\ntrain_losses, val_losses, train_ious, val_ious = [], [], [], []\n\nnum_epochs = 12\nimage_width, image_height = 224, 224  # Примерные размеры изображения, измените на ваши\nbest_val_loss = float('inf')\nfor epoch in range(num_epochs):\n    model.train()\n    train_mse, train_iou_accum = 0, 0\n    print(f'Epoch: {epoch}')\n    \n    #epoch_start = time.time()\n    #previous_loop_end_time = time.time()\n    for batch_idx, (images, targets) in enumerate(train_loader):\n        #for_line_time = time.time() - previous_loop_end_time\n        #load_time_start = time.time()\n\n        \n        if torch.cuda.is_available():\n            images, targets = images.cuda(), targets.cuda()\n        \n        #load_time = time.time() - load_time_start\n        \n        #process_start = time.time()\n        optimizer.zero_grad()\n        outputs = model(images)\n        \n        loss = criterion(outputs, targets)  # Считаем потери для каждого временного шага\n        \n        loss.backward()  # Обратное распространение ошибки\n        \n        train_mse += loss.item()\n        \n        #backward_start = time.time()\n        optimizer.step()\n        # backward_time = time.time() - backward_start\n        \n        #detach_start = time.time()\n        outputs = outputs.detach().cpu()\n        targets = targets.detach().cpu()\n        #detach_time = time.time() - detach_start\n        \n        #iou_start = time.time()\n        predicted_corners = convert_yolo_to_corners(outputs, image_width, image_height)\n        target_corners = convert_yolo_to_corners(targets.squeeze(), image_width, image_height)\n        \n        iou_scores = compute_iou_batch(predicted_corners, target_corners)\n        average_iou = iou_scores.mean().item()\n        train_iou_accum += average_iou\n        #iou_time = time.time() - iou_start\n        \n        #total_time = time.time() - process_start\n        #print(f'Batch {batch_idx} processed in {total_time:.2f}s (Load: {load_time:.2f}s, Backward: {backward_time:.2f}s, Detach: {detach_time:.2f}s, IoU: {iou_time:.2f}s, For Loop line took: {for_line_time:.2f}s)')\n        \n        #previous_loop_end_time = time.time()\n    \n    #epoch_time = time.time() - epoch_start\n    #print(f'Epoch {epoch} completed in {epoch_time:.2f}s')\n\n    average_train_mse = train_mse / len(train_loader)\n    average_train_iou = train_iou_accum / len(train_loader)\n    train_losses.append(average_train_mse)\n    train_ious.append(average_train_iou)\n\n    model.eval()\n    val_mse, val_iou_accum = 0, 0\n    with torch.no_grad():\n        for images, targets in test_loader:\n            if torch.cuda.is_available():\n                images, targets = images.cuda(), targets.cuda()\n\n            outputs = model(images)\n            confidences, bboxes = get_confidence_and_bbox(outputs)\n\n            try:\n                mask = confidences >= confidence_threshold\n                if last_confident_bbox is not None and (~mask).any():\n                    bboxes[~mask, :] = last_confident_bbox\n\n                if mask.any():\n                    last_confident_bbox = bboxes[mask][-1]\n            except IndexError as e:\n                print(f\"Error processing bboxes: {str(e)}\")\n\n            loss = criterion(outputs, targets)\n            val_mse += loss.item()\n\n            predicted_corners = convert_yolo_to_corners(bboxes, image_width, image_height)\n            target_corners = convert_yolo_to_corners(targets.squeeze(), image_width, image_height)\n            iou_scores = compute_iou_batch(predicted_corners, target_corners)\n            val_iou_accum += iou_scores.mean().item()\n\n\n    average_val_loss = val_mse / len(test_loader)\n    average_val_iou = val_iou_accum / len(test_loader)\n    val_losses.append(average_val_loss)\n    val_ious.append(average_val_iou)\n    \n    f val_losses[-1] < best_val_loss:\n        best_val_loss = val_losses[-1]\n        save_checkpoint({\n            'epoch': epoch + 1,\n            'state_dict': model.state_dict(),\n            'optimizer': optimizer.state_dict(),\n            'loss': best_val_loss,\n        }, filename=f\"best_model_epoch_{epoch+1}.pth\")\n\n    # Отрисовка графиков\n    plot_metrics(train_losses, val_losses, train_ious, val_ious, epoch)\n\n    print(f\"Epoch {epoch + 1}, Train Loss: {average_train_mse}, Train IoU: {average_train_iou}, Val Loss: {average_val_loss}, Val IoU: {average_val_iou}\")\n\n# Визуализация потерь и IoU\nepochs = range(1, num_epochs + 1)\nplt.figure(figsize=(12, 6))\n\nplt.subplot(1, 2, 1)\nplt.plot(epochs, train_losses, label='Training Loss')\nplt.plot(epochs, val_losses, label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs, train_ious, label='Training IoU')\nplt.plot(epochs, val_ious, label='Validation IoU')\nplt.title('Training and Validation IoU')\nplt.xlabel('Epoch')\nplt.ylabel('IoU')\nplt.legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:18:53.682045Z","iopub.execute_input":"2024-05-06T19:18:53.682429Z","iopub.status.idle":"2024-05-06T19:19:44.274862Z","shell.execute_reply.started":"2024-05-06T19:18:53.682399Z","shell.execute_reply":"2024-05-06T19:19:44.273380Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_names = [img_name for img_name in os.listdir(img_dir) if img_name.endswith('.jpg')]\nimg_names.sort(key=lambda x: (x.split('_')[0], int(x.split('_')[2]), int(x.split('_')[3].split('.')[0])))\n\n\nprint(img_names)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T18:58:20.858057Z","iopub.execute_input":"2024-05-06T18:58:20.858794Z","iopub.status.idle":"2024-05-06T18:58:21.025930Z","shell.execute_reply.started":"2024-05-06T18:58:20.858757Z","shell.execute_reply":"2024-05-06T18:58:21.024969Z"},"trusted":true},"execution_count":null,"outputs":[]}]}