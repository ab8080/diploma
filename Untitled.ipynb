{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8323525,"sourceType":"datasetVersion","datasetId":4944350}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.utils.data import Dataset\nimport os\nfrom PIL import Image\nfrom torchvision import transforms","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-05T20:08:04.080175Z","iopub.execute_input":"2024-05-05T20:08:04.080897Z","iopub.status.idle":"2024-05-05T20:08:10.331012Z","shell.execute_reply.started":"2024-05-05T20:08:04.080866Z","shell.execute_reply":"2024-05-05T20:08:10.329990Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomLoss(nn.Module):\n    def __init__(self):\n        super(CustomLoss, self).__init__()\n        self.mse_loss = nn.MSELoss(reduction='sum')  # Для координат\n        self.bce_loss = nn.BCEWithLogitsLoss(reduction='sum')  # Для уверенности\n\n    def forward(self, predictions, targets):\n        # Предсказанные уверенности и координаты\n        pred_confidences = predictions[:, 0]\n        pred_boxes = predictions[:, 1:]\n\n        # Целевые уверенности (все 1, так как все целевые боксы содержат объекты)\n        target_confidences = torch.ones_like(pred_confidences)\n\n        # Целевые координаты\n        target_boxes = targets\n\n        # Расчет потерь\n        confidence_loss = self.bce_loss(pred_confidences, target_confidences)\n        coordinate_loss = self.mse_loss(pred_boxes, target_boxes)\n\n        return confidence_loss + coordinate_loss","metadata":{"execution":{"iopub.status.busy":"2024-05-05T20:08:10.333230Z","iopub.execute_input":"2024-05-05T20:08:10.333733Z","iopub.status.idle":"2024-05-05T20:08:10.341994Z","shell.execute_reply.started":"2024-05-05T20:08:10.333698Z","shell.execute_reply":"2024-05-05T20:08:10.341026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Определение Symmetrical ReLU (SymReLU)\nclass SymReLU(nn.Module):\n    def __init__(self, alpha=1.0):\n        super(SymReLU, self).__init__()\n        self.alpha = alpha\n    \n    def forward(self, x):\n        return torch.max(x, torch.zeros_like(x)) - self.alpha * torch.min(x, torch.zeros_like(x))\n\n# Определение слоя свертки с SymReLU\nclass ConvSymRelu(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, groups=1):\n        super(ConvSymRelu, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, groups=groups, bias=False)\n        self.symrelu = SymReLU(alpha=1.0)\n    \n    def forward(self, x):\n        return self.symrelu(self.conv(x))\n    \nclass VideoYOLOv3(nn.Module):\n    def __init__(self, num_classes, lstm_hidden_size=512, lstm_layers=1):\n        super(VideoYOLOv3, self).__init__()\n        self.conv_layers = nn.Sequential(\n            ConvSymRelu(3, 12, (5, 5), (1, 1), (2, 2)),\n            nn.MaxPool2d((2, 2), stride=(2, 2)),\n            ConvSymRelu(12, 16, (5, 5), (2, 2), (1, 1)),\n            ConvSymRelu(16, 16, (3, 3), (2, 2), (1, 1)),\n            ConvSymRelu(16, 16, (3, 3), (1, 1), (1, 1)),\n            nn.MaxPool2d((2, 2), stride=(2, 2)),\n            ConvSymRelu(16, 24, (3, 3), (1, 1), (1, 1)),\n            ConvSymRelu(24, 48, (3, 3), (2, 2), (1, 1)),\n            ConvSymRelu(48, 48, (3, 3), (1, 1), (1, 1)),\n            ConvSymRelu(48, 48, (3, 3), (1, 1), (1, 1)),\n            ConvSymRelu(48, 48, (3, 3), (1, 1), (1, 1)),\n            ConvSymRelu(48, 64, (3, 3), (1, 1), (1, 1)),\n            ConvSymRelu(64, 64, (7, 7), (7, 7), (0, 0)),  # Уменьшаем до 1x1\n            ConvSymRelu(64, 4, (1, 1), (1, 1), (0, 0)),  # Меняем количество каналов, сохраняем 1x1\n            ConvSymRelu(4, 4, (3, 3), (1, 1), (1, 1)),\n            ConvSymRelu(4, 4, (3, 3), (1, 1), (1, 1)),\n            nn.Upsample(scale_factor=(1, 1), mode='nearest'),\n            ConvSymRelu(4, 96, (3, 3), (1, 1), (1, 1)),\n            ConvSymRelu(96, 256, (1, 1), (1, 1), (0, 0)),\n            nn.Conv2d(256, 512, (1, 1))\n        )\n        \n        # LSTM слой\n        self.lstm = nn.LSTM(input_size=512,  # Размер входа должен соответствовать выходу последнего сверточного слоя\n                            hidden_size=lstm_hidden_size,\n                            num_layers=lstm_layers,\n                            batch_first=True)\n        \n        self.fc = nn.Linear(lstm_hidden_size, 4)\n\n    def forward(self, x):\n        batch_size, timesteps, C, H, W = x.size()\n        c_in = x.view(batch_size * timesteps, C, H, W)\n        c_out = self.conv_layers(c_in)\n        \n        # Переводим выход сверточных слоев в формат (batch, timesteps, features)\n        c_out = c_out.view(batch_size, timesteps, -1)\n        \n        # LSTM слой теперь обрабатывает каждый кадр\n        r_out, _ = self.lstm(c_out)\n        \n        # Применяем полносвязный слой к каждому временному шагу\n        out = self.fc(r_out)\n        \n        return out\n","metadata":{"execution":{"iopub.status.busy":"2024-05-05T20:08:10.343391Z","iopub.execute_input":"2024-05-05T20:08:10.344366Z","iopub.status.idle":"2024-05-05T20:08:10.367342Z","shell.execute_reply.started":"2024-05-05T20:08:10.344331Z","shell.execute_reply":"2024-05-05T20:08:10.366457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def annotations_to_tensor(ann_file):\n    with open(ann_file, 'r') as file:\n        annotations = file.readlines()\n    \n    tensor_annotations = []\n    for ann_string in annotations:\n        ann_data = ann_string.strip().split()\n        coords = list(map(float, ann_data[1:]))  # Преобразуем координаты во float (пропускаем класс)\n        tensor_annotations.append(coords)\n    \n    return torch.tensor(tensor_annotations, dtype=torch.float32)  # Преобразование списка в тензор\n","metadata":{"execution":{"iopub.status.busy":"2024-05-05T20:08:10.369815Z","iopub.execute_input":"2024-05-05T20:08:10.370092Z","iopub.status.idle":"2024-05-05T20:08:10.381063Z","shell.execute_reply.started":"2024-05-05T20:08:10.370069Z","shell.execute_reply":"2024-05-05T20:08:10.380156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class VideoFramesDataset(Dataset):\n    def __init__(self, video_dir, transform=None):\n        \"\"\"\n        Конструктор класса VideoFramesDataset.\n\n        Параметры:\n        video_dir (str): Путь к каталогу с видео, каждое видео в отдельной папке.\n        transform (callable, optional): Преобразования, применяемые к каждому кадру.\n        \"\"\"\n        self.video_dir = video_dir\n        self.transform = transform\n\n        # Список всех видео (папок)\n        self.videos = [os.path.join(video_dir, v) for v in os.listdir(video_dir) if os.path.isdir(os.path.join(video_dir, v))]\n        self.videos.sort()\n\n    def __len__(self):\n        return len(self.videos)\n\n    def __getitem__(self, idx):\n        video_path = self.videos[idx]\n        images_path = os.path.join(video_path, 'images')\n        annotations_path = os.path.join(video_path, 'annotations')\n\n        frames = [os.path.join(images_path, frame) for frame in sorted(os.listdir(images_path)) if frame.endswith('.jpg')]\n        annotations_files = [os.path.join(annotations_path, ann) for ann in sorted(os.listdir(annotations_path)) if ann.endswith('.txt')]\n\n        images = [Image.open(frame).convert('RGB') for frame in frames]\n        anns = [annotations_to_tensor(ann_file) for ann_file in annotations_files]\n\n        if self.transform:\n            images = [self.transform(image) for image in images]\n\n        images = torch.stack(images)\n        annotations = torch.cat(anns, dim=0)  # Объединяем все аннотации в один тензор\n\n        return images, annotations\n\n\n    def load_annotations(self, ann_file):\n        \"\"\"\n        Загрузка аннотаций из файла. Здесь необходимо адаптировать под формат ваших данных.\n        \"\"\"\n        with open(ann_file, 'r') as file:\n            annotations = file.readlines()  # Простой пример, может потребоваться изменение\n        return annotations","metadata":{"execution":{"iopub.status.busy":"2024-05-05T20:08:10.382470Z","iopub.execute_input":"2024-05-05T20:08:10.382860Z","iopub.status.idle":"2024-05-05T20:08:10.397524Z","shell.execute_reply.started":"2024-05-05T20:08:10.382830Z","shell.execute_reply":"2024-05-05T20:08:10.396606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Пример использования\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor()\n])\n\ntest_dataset = VideoFramesDataset(\"/kaggle/input/dataset0505/aaa/test\", transform=transform)\ntrain_dataset = VideoFramesDataset(\"/kaggle/input/dataset0505/aaa/train\", transform=transform)\nprint(\"Количество видео в датасете:\", len(test_dataset))\n\n# Получаем кадры и аннотации первого видео\nvideo_frames, video_annotations = test_dataset[0]\nprint(\"Форма тензора с кадрами первого видео:\", video_frames.shape)\nprint(\"Аннотации первого видео:\", video_annotations)","metadata":{"execution":{"iopub.status.busy":"2024-05-05T20:08:10.398767Z","iopub.execute_input":"2024-05-05T20:08:10.399133Z","iopub.status.idle":"2024-05-05T20:08:16.884016Z","shell.execute_reply.started":"2024-05-05T20:08:10.399101Z","shell.execute_reply":"2024-05-05T20:08:16.882964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class VideoBBoxLoss(nn.Module):\n    def __init__(self):\n        super(VideoBBoxLoss, self).__init__()\n        self.mse_loss = nn.MSELoss(reduction='mean')  # Среднее по всем элементам\n\n    def forward(self, predictions, targets):\n        # Предсказания и цели имеют размерность [batch_size, seq_len, 4]\n        # где 4 соответствует [x_center, y_center, width, height]\n        return self.mse_loss(predictions, targets)\n\n# Создание экземпляра функции потерь\ncriterion = VideoBBoxLoss()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-05T20:08:16.887222Z","iopub.execute_input":"2024-05-05T20:08:16.887513Z","iopub.status.idle":"2024-05-05T20:08:16.894325Z","shell.execute_reply.started":"2024-05-05T20:08:16.887487Z","shell.execute_reply":"2024-05-05T20:08:16.893088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\n# Создаем DataLoader для тестового датасета\ntest_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\ntrain_loader = DataLoader(train_dataset, batch_size=1, shuffle=False)\ntest_loader","metadata":{"execution":{"iopub.status.busy":"2024-05-05T20:08:16.895629Z","iopub.execute_input":"2024-05-05T20:08:16.895945Z","iopub.status.idle":"2024-05-05T20:08:16.907953Z","shell.execute_reply.started":"2024-05-05T20:08:16.895920Z","shell.execute_reply":"2024-05-05T20:08:16.906944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Предполагается, что модель и DataLoader уже определены\nmodel = VideoYOLOv3(num_classes=1)  # Подставьте корректное количество классов\nif torch.cuda.is_available():\n    model = model.cuda()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.002)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-05T20:08:27.598477Z","iopub.execute_input":"2024-05-05T20:08:27.598871Z","iopub.status.idle":"2024-05-05T20:08:27.900160Z","shell.execute_reply.started":"2024-05-05T20:08:27.598841Z","shell.execute_reply":"2024-05-05T20:08:27.899099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt\nfrom torchvision.ops import box_iou","metadata":{"execution":{"iopub.status.busy":"2024-05-05T20:08:30.191096Z","iopub.execute_input":"2024-05-05T20:08:30.191965Z","iopub.status.idle":"2024-05-05T20:08:30.196733Z","shell.execute_reply.started":"2024-05-05T20:08:30.191925Z","shell.execute_reply":"2024-05-05T20:08:30.195504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def batch_iou(bboxes_pred, bboxes_gt):\n    ious = []\n    for pred, gt in zip(bboxes_pred, bboxes_gt):\n        iou = box_iou(pred.unsqueeze(0), gt.unsqueeze(0))\n        ious.append(iou)\n    return torch.tensor(ious)  # Возвращаем тензор IoU для каждого кадра","metadata":{"execution":{"iopub.status.busy":"2024-05-05T20:08:32.121080Z","iopub.execute_input":"2024-05-05T20:08:32.121446Z","iopub.status.idle":"2024-05-05T20:08:32.126973Z","shell.execute_reply.started":"2024-05-05T20:08:32.121414Z","shell.execute_reply":"2024-05-05T20:08:32.125843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_yolo_to_corners(bboxes, img_width, img_height):\n    # Предполагаем, что bboxes имеет размерность [batch_size, seq_len, 4]\n    batch_size, seq_len, _ = bboxes.shape\n    converted_bboxes = []\n    \n    for i in range(batch_size):\n        video_bboxes = []\n        for j in range(seq_len):\n            x_center, y_center, width, height = bboxes[i, j]\n            x_min = (x_center - width / 2) * img_width\n            y_min = (y_center - height / 2) * img_height\n            x_max = (x_center + width / 2) * img_width\n            y_max = (y_center + height / 2) * img_height\n            video_bboxes.append([x_min, y_min, x_max, y_max])\n        converted_bboxes.append(video_bboxes)\n    \n    # Конвертация в тензор с той же размерностью, что и исходный bboxes тензор\n    return torch.tensor(converted_bboxes, dtype=torch.float32)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-05T20:08:34.250390Z","iopub.execute_input":"2024-05-05T20:08:34.251209Z","iopub.status.idle":"2024-05-05T20:08:34.258399Z","shell.execute_reply.started":"2024-05-05T20:08:34.251177Z","shell.execute_reply":"2024-05-05T20:08:34.257437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nfrom torchvision.ops import box_iou\n\ntrain_ious, val_ious = [], []\n\nnum_epochs = 12\nimage_width, image_height = 224, 224  # Примерные размеры изображения\n\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss_accum, train_iou_accum = 0, 0\n    print(f'Epoch: {epoch}')\n\n    for batch_idx, (images, targets) in enumerate(train_loader):\n        if torch.cuda.is_available():\n            images, targets = images.cuda(), targets.cuda()\n        \n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        train_loss_accum += loss.item()\n        \n        outputs = outputs.detach().cpu()\n        targets = targets.detach().cpu()\n        \n        # Расчет IoU для каждого батча\n\n        predicted_corners = convert_yolo_to_corners(outputs, image_width, image_height)\n        target_corners = convert_yolo_to_corners(targets, image_width, image_height)\n\n        for i in range(predicted_corners.size(0)):\n            iou_scores = box_iou(predicted_corners[i], target_corners[i])\n            train_iou_accum += iou_scores.diag().mean().item()\n\n    average_train_iou = train_iou_accum / len(train_loader)\n    train_ious.append(average_train_iou)\n\n    model.eval()\n    val_mse, val_iou_accum = 0, 0\n    with torch.no_grad():\n        for images, targets in test_loader:\n            if torch.cuda.is_available():\n                images, targets = images.cuda(), targets.cuda()\n\n            outputs = model(images)\n            outputs = outputs.detach().cpu()\n            targets = targets.detach().cpu()\n\n\n            # Расчет IoU\n            predicted_corners = convert_yolo_to_corners(outputs, image_width, image_height)\n            target_corners = convert_yolo_to_corners(targets, image_width, image_height)\n            for i in range(predicted_corners.size(0)):\n                iou_scores = box_iou(predicted_corners[i], target_corners[i])\n                val_iou_accum += iou_scores.diag().mean().item()\n\n    average_val_iou = val_iou_accum / len(test_loader)\n    val_ious.append(average_val_iou)\n\n    print(f\"Epoch {epoch + 1}, Train IoU: {average_train_iou}, Val IoU: {average_val_iou}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-05T20:09:00.448044Z","iopub.execute_input":"2024-05-05T20:09:00.448449Z","iopub.status.idle":"2024-05-05T20:11:44.678324Z","shell.execute_reply.started":"2024-05-05T20:09:00.448406Z","shell.execute_reply":"2024-05-05T20:11:44.676931Z"},"trusted":true},"execution_count":null,"outputs":[]}]}