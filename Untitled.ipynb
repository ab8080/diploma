{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8103326,"sourceType":"datasetVersion","datasetId":4785687}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.utils.data import Dataset\nimport os\nfrom PIL import Image","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-06T18:47:27.793291Z","iopub.execute_input":"2024-05-06T18:47:27.793701Z","iopub.status.idle":"2024-05-06T18:47:27.798885Z","shell.execute_reply.started":"2024-05-06T18:47:27.793661Z","shell.execute_reply":"2024-05-06T18:47:27.797757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomLoss(nn.Module):\n    def __init__(self):\n        super(CustomLoss, self).__init__()\n        self.mse_loss = nn.MSELoss(reduction='sum')  # Для координат\n        self.bce_loss = nn.BCEWithLogitsLoss(reduction='sum')  # Для уверенности\n\n    def forward(self, predictions, targets):\n        # Плоское представление для упрощения операций\n        # predictions shape: [batch_size, seq_len, 5] \n        # targets shape: [batch_size, seq_len, 1, 5]\n        predictions = predictions.view(-1, 5)  # Сглаживаем батч и временные шаги\n        targets = targets.view(-1, 5)  # Сглаживаем батч, временные шаги и учитываем, что targets уже содержат 5 значений\n\n        # Предсказанные уверенности и координаты\n        pred_confidences = predictions[:, 0]  # Первый столбец - уверенность\n        pred_boxes = predictions[:, 1:]  # Остальные столбцы - координаты\n\n        # Целевые уверенности (последний столбец в targets)\n        target_confidences = targets[:, 0]  # Используем реальные значения уверенности из targets\n\n        # Целевые координаты\n        target_boxes = targets[:, 1:]  # Пропускаем столбец уверенности в targets\n\n        # Расчет потерь\n        confidence_loss = self.bce_loss(pred_confidences, target_confidences)\n        coordinate_loss = self.mse_loss(pred_boxes, target_boxes)  # Совпадение размеров уже гарантировано\n\n        # Возвращаем сумму потерь\n        return confidence_loss + coordinate_loss\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T18:47:27.800934Z","iopub.execute_input":"2024-05-06T18:47:27.801203Z","iopub.status.idle":"2024-05-06T18:47:27.811806Z","shell.execute_reply.started":"2024-05-06T18:47:27.801180Z","shell.execute_reply":"2024-05-06T18:47:27.810999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Определение Symmetrical ReLU (SymReLU)\nclass SymReLU(nn.Module):\n    def __init__(self, alpha=1.0):\n        super(SymReLU, self).__init__()\n        self.alpha = alpha\n    \n    def forward(self, x):\n        return torch.max(x, torch.zeros_like(x)) - self.alpha * torch.min(x, torch.zeros_like(x))\n\n# Определение слоя свертки с SymReLU\nclass ConvSymRelu(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, groups=1):\n        super(ConvSymRelu, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, groups=groups, bias=False)\n        self.symrelu = SymReLU(alpha=1.0)\n    \n    def forward(self, x):\n        return self.symrelu(self.conv(x))\n    \nclass NewYOLOv3(nn.Module):\n    def __init__(self, num_classes, lstm_hidden_size=512, lstm_layers=1, seq_len=8):\n        super(NewYOLOv3, self).__init__()\n        # Определение сверточных слоев\n        self.conv0_yolo = ConvSymRelu(3, 12, (5, 5), (1, 1), (2, 2))\n        self.maxpool0 = nn.MaxPool2d((2, 2), stride=(2, 2))\n        self.conv1 = ConvSymRelu(12, 16, (5, 5), (2, 2), (1, 1))\n        self.conv2 = ConvSymRelu(16, 16, (3, 3), (2, 2), (1, 1))\n        self.conv3 = ConvSymRelu(16, 16, (3, 3), (1, 1), (1, 1))\n        self.pool4 = nn.MaxPool2d((2, 2), stride=(2, 2))\n        self.conv5 = ConvSymRelu(16, 24, (3, 3), (1, 1), (1, 1))\n        self.conv6 = ConvSymRelu(24, 48, (3, 3), (2, 2), (1, 1))\n        self.conv7 = ConvSymRelu(48, 48, (3, 3), (1, 1), (1, 1))\n        self.conv8 = ConvSymRelu(48, 48, (3, 3), (1, 1), (1, 1))\n        self.conv9 = ConvSymRelu(48, 48, (3, 3), (1, 1), (1, 1))\n        self.conv11 = ConvSymRelu(48, 64, (3, 3), (1, 1), (1, 1))\n        self.conv13 = ConvSymRelu(64, 64, (7, 7), (7, 7), (0, 0))  # Уменьшаем до 1x1\n        self.conv14 = ConvSymRelu(64, 4, (1, 1), (1, 1), (0, 0))  # Меняем количество каналов, сохраняем 1x1\n        self.conv14_1 = ConvSymRelu(4, 4, (3, 3), (1, 1), (1, 1))\n        self.conv14_2 = ConvSymRelu(4, 4, (3, 3), (1, 1), (1, 1))\n        self.upsample0 = nn.Upsample(scale_factor=(1, 1), mode='nearest')\n        self.conv15 = ConvSymRelu(4, 96, (3, 3), (1, 1), (1, 1))\n        self.prefinal = ConvSymRelu(96, 512, (1, 1), (1, 1), (0, 0))\n        self.final = nn.Conv2d(512, 512, (1, 1))\n        \n        # Определение GRU слоя\n        self.gru = nn.GRU(input_size=512, hidden_size=lstm_hidden_size, num_layers=lstm_layers, batch_first=True)\n        self.fc = nn.Linear(lstm_hidden_size, 5)\n        self.seq_len = seq_len\n\n    def forward(self, x):\n\n        batch_size, seq_len, C, H, W = x.size()\n        # Сверточная часть\n        x = x.view(batch_size * seq_len, C, H, W)  \n        x = self.conv0_yolo(x)\n        x = self.maxpool0(x)\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.pool4(x)\n        x = self.conv5(x)\n        x = self.conv6(x)\n        x = self.conv7(x)\n        x = self.conv8(x)\n        x = self.conv9(x)\n        x = self.conv11(x)\n        x = self.conv13(x)\n        x = self.conv14(x)\n        x = self.conv14_1(x)\n        x = self.conv14_2(x)\n        x = self.upsample0(x)\n        x = self.conv15(x)\n        x = self.prefinal(x)\n        x = self.final(x)\n        x = x.view(batch_size, seq_len, -1)  # Подготовка входа для GRU\n\n        # Рекуррентная часть\n        hidden = None  # GRU сам инициализирует скрытое состояние, если не задано\n        x, hidden = self.gru(x) # возможно надо убрать hidden\n        # Применение последнего слоя\n        x = self.fc(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-05-06T18:47:27.812960Z","iopub.execute_input":"2024-05-06T18:47:27.813247Z","iopub.status.idle":"2024-05-06T18:47:27.838743Z","shell.execute_reply.started":"2024-05-06T18:47:27.813212Z","shell.execute_reply":"2024-05-06T18:47:27.837724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class YourDataset(Dataset):\n    def __init__(self, img_dir, ann_dir, transform=None):\n        self.img_dir = img_dir\n        self.ann_dir = ann_dir\n        self.transform = transform\n\n        # Получение списка имен файлов изображений\n        self.img_names = [img_name for img_name in os.listdir(img_dir) if img_name.endswith('.jpg')]\n        self.img_names.sort(key=lambda x: (x.split('_')[0], int(x.split('_')[2]), int(x.split('_')[3].split('.')[0])))\n\n        # Группировка по видео\n        self.grouped_images = {}\n        for img_name in self.img_names:\n            key = '_'.join(img_name.split('_')[:3])  # a_b_c\n            if key not in self.grouped_images:\n                self.grouped_images[key] = []\n            self.grouped_images[key].append(img_name)\n\n        # Отбрасываем группы, которые не могут полностью сформировать минибатчи\n        self.final_img_names = []\n        for key, imgs in self.grouped_images.items():\n            if len(imgs) >= 8:\n                self.final_img_names.extend(imgs[:len(imgs) - (len(imgs) % 8)])\n\n    def __len__(self):\n        return len(self.final_img_names) // 8\n\n    def __getitem__(self, idx):\n        images = []\n        annotations = []\n        start_index = idx * 8\n        for i in range(8):\n            img_name = self.final_img_names[start_index + i]\n            img_path = os.path.join(self.img_dir, img_name)\n            ann_path = os.path.join(self.ann_dir, img_name.replace('.jpg', '.txt'))\n\n            image = Image.open(img_path).convert('RGB')\n            ann = self.load_annotations(ann_path)\n\n            if self.transform:\n                image = self.transform(image)\n\n            images.append(image)\n            annotations.append(ann)\n            \n        images = torch.stack(images)  # Преобразование списка изображений в тензор\n        return images, torch.stack(annotations)\n\n    @staticmethod\n    def load_annotations(ann_path):\n        annotations = []\n        with open(ann_path, 'r') as file:\n            for line in file:\n                _, x_center, y_center, width, height = map(float, line.split())\n                annotations.append([1, x_center, y_center, width, height])\n        return torch.tensor(annotations)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:17:30.090637Z","iopub.execute_input":"2024-05-06T19:17:30.091050Z","iopub.status.idle":"2024-05-06T19:17:30.107837Z","shell.execute_reply.started":"2024-05-06T19:17:30.091016Z","shell.execute_reply":"2024-05-06T19:17:30.106568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision import transforms\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\nfrom torchvision.ops import box_iou\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T18:48:29.599083Z","iopub.execute_input":"2024-05-06T18:48:29.599471Z","iopub.status.idle":"2024-05-06T18:48:29.605524Z","shell.execute_reply.started":"2024-05-06T18:48:29.599440Z","shell.execute_reply":"2024-05-06T18:48:29.604550Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Теперь пути к данным должны указывать на распакованные каталоги\nimg_dir = '/kaggle/input/brikerdataset/dataset0904/train_images'  # Пример пути к изображениям для обучения\nann_dir = '/kaggle/input/brikerdataset/dataset0904/train_annotations'  # Пример пути к аннотациям для обучения\n\ntest_img_dir = '/kaggle/input/brikerdataset/dataset0904/test_images'  # Пример пути к изображениям для тестирования\ntest_ann_dir = '/kaggle/input/brikerdataset/dataset0904/test_annotations'  # Пример пути к аннотациям для тестирования","metadata":{"execution":{"iopub.status.busy":"2024-05-06T18:47:28.071251Z","iopub.execute_input":"2024-05-06T18:47:28.071837Z","iopub.status.idle":"2024-05-06T18:47:28.076087Z","shell.execute_reply.started":"2024-05-06T18:47:28.071810Z","shell.execute_reply":"2024-05-06T18:47:28.075172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size=16\nseq_len=8","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:18:34.662831Z","iopub.execute_input":"2024-05-06T19:18:34.663241Z","iopub.status.idle":"2024-05-06T19:18:34.668737Z","shell.execute_reply.started":"2024-05-06T19:18:34.663208Z","shell.execute_reply":"2024-05-06T19:18:34.667704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:13:41.935065Z","iopub.execute_input":"2024-05-06T19:13:41.935484Z","iopub.status.idle":"2024-05-06T19:13:41.945659Z","shell.execute_reply.started":"2024-05-06T19:13:41.935451Z","shell.execute_reply":"2024-05-06T19:13:41.944723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Создание экземпляра новой модели\nmodel = NewYOLOv3(num_classes=1)  # Обратите внимание на количество классов\n\n# Проверка доступности CUDA\nif torch.cuda.is_available():\n    model = model.cuda()\n\n# Создание экземпляра функции потерь и оптимизатора\ncriterion = CustomLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# Подготовка датасетов, загрузчиков и трансформаций, если они нужны\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor()\n])\n\ntrain_dataset = YourDataset(img_dir, ann_dir, transform=transform)\ntest_dataset = YourDataset(test_img_dir, test_ann_dir, transform=transform)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, num_workers=1, pin_memory=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=1, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:18:39.269613Z","iopub.execute_input":"2024-05-06T19:18:39.270363Z","iopub.status.idle":"2024-05-06T19:18:39.550439Z","shell.execute_reply.started":"2024-05-06T19:18:39.270329Z","shell.execute_reply":"2024-05-06T19:18:39.549424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_yolo_to_corners(bboxes, img_width, img_height):\n    # Предполагаем, что размеры bboxes: [batch_size, seq_len, 5]\n    # Первый элемент - confidence, игнорируем его при конвертации координат.\n    batch_size, seq_len, _ = bboxes.shape\n    converted_bboxes = []\n    for batch in range(batch_size):\n        batch_converted_bboxes = []\n        for seq in range(seq_len):\n            # Берем только координаты x_center, y_center, width, height\n            x_center, y_center, width, height = bboxes[batch, seq, 1:5]\n            x_min = (x_center - width / 2) * img_width\n            y_min = (y_center - height / 2) * img_height\n            x_max = (x_center + width / 2) * img_width\n            y_max = (y_center + height / 2) * img_height\n            batch_converted_bboxes.append([x_min, y_min, x_max, y_max])\n        converted_bboxes.append(batch_converted_bboxes)\n    return torch.tensor(converted_bboxes, dtype=torch.float32)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T18:47:28.276462Z","iopub.execute_input":"2024-05-06T18:47:28.276779Z","iopub.status.idle":"2024-05-06T18:47:28.283981Z","shell.execute_reply.started":"2024-05-06T18:47:28.276756Z","shell.execute_reply":"2024-05-06T18:47:28.282973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_confidence_and_bbox(outputs):\n    \"\"\"\n    Разделяет выходные данные модели на уверенности и координаты bounding boxes.\n\n    Параметры:\n        outputs (torch.Tensor): Тензор с выходными данными модели размерности [batch_size, 5],\n                                где каждая строка содержит [confidence, x, y, width, height].\n\n    Возвращает:\n        Tuple[torch.Tensor, torch.Tensor]:\n        - Вектор уверенностей размерности [batch_size],\n        - Матрицу координат bounding boxes размерности [batch_size, 4].\n    \"\"\"\n    # Предполагаем, что outputs уже находится на CPU, если необходимо\n    confidences = outputs[:, 0]  # Все строки, первый столбец\n    bboxes = outputs[:, 1:]  # Все строки, со второго столбца до последнего\n\n    return confidences, bboxes","metadata":{"execution":{"iopub.status.busy":"2024-05-06T18:47:28.284894Z","iopub.execute_input":"2024-05-06T18:47:28.285148Z","iopub.status.idle":"2024-05-06T18:47:28.296587Z","shell.execute_reply.started":"2024-05-06T18:47:28.285126Z","shell.execute_reply":"2024-05-06T18:47:28.295683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_iou_batch(predicted_corners, target_corners):\n    # Подготовка тензоров для функции box_iou\n    # predicted_corners и target_corners должны иметь форму [batch_size, seq_len, 4]\n    batch_size, seq_len, _ = predicted_corners.shape\n    ious = torch.zeros(batch_size, seq_len)  # Тензор для хранения значений IoU\n\n    # Вычисляем IoU для каждой пары предсказанных и целевых углов в батче\n    for batch in range(batch_size):\n        for seq in range(seq_len):\n            # Переформатирование тензоров для соблюдения ожидаемой размерности в box_iou\n            pred_box = predicted_corners[batch, seq].unsqueeze(0)\n            target_box = target_corners[batch, seq].unsqueeze(0)\n            iou = box_iou(pred_box, target_box)\n            ious[batch, seq] = iou  # Запись результата IoU в тензор\n\n    return ious","metadata":{"execution":{"iopub.status.busy":"2024-05-06T18:47:28.297623Z","iopub.execute_input":"2024-05-06T18:47:28.297885Z","iopub.status.idle":"2024-05-06T18:47:28.309793Z","shell.execute_reply.started":"2024-05-06T18:47:28.297862Z","shell.execute_reply":"2024-05-06T18:47:28.308972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport time\nconfidence_threshold = 0.50  # Порог уверенности для демонстрации\nlast_confident_bbox = None  # Для хранения последнего уверенного bounding box\n\ntrain_losses, val_losses, train_ious, val_ious = [], [], [], []\n\nnum_epochs = 12\nimage_width, image_height = 224, 224  # Примерные размеры изображения, измените на ваши\n\nfor epoch in range(num_epochs):\n    model.train()\n    train_mse, train_iou_accum = 0, 0\n    print(f'Epoch: {epoch}')\n    \n    #epoch_start = time.time()\n    #previous_loop_end_time = time.time()\n    for batch_idx, (images, targets) in enumerate(train_loader):\n        #for_line_time = time.time() - previous_loop_end_time\n        #load_time_start = time.time()\n\n        \n        if torch.cuda.is_available():\n            images, targets = images.cuda(), targets.cuda()\n        \n        #load_time = time.time() - load_time_start\n        \n        #process_start = time.time()\n        optimizer.zero_grad()\n        outputs = model(images)\n        \n        loss = criterion(outputs, targets)  # Считаем потери для каждого временного шага\n        \n        loss.backward()  # Обратное распространение ошибки\n        \n        train_mse += loss.item()\n        \n        #backward_start = time.time()\n        optimizer.step()\n        # backward_time = time.time() - backward_start\n        \n        #detach_start = time.time()\n        outputs = outputs.detach().cpu()\n        targets = targets.detach().cpu()\n        #detach_time = time.time() - detach_start\n        \n        #iou_start = time.time()\n        predicted_corners = convert_yolo_to_corners(outputs, image_width, image_height)\n        target_corners = convert_yolo_to_corners(targets.squeeze(), image_width, image_height)\n        \n        iou_scores = compute_iou_batch(predicted_corners, target_corners)\n        average_iou = iou_scores.mean().item()\n        train_iou_accum += average_iou\n        #iou_time = time.time() - iou_start\n        \n        #total_time = time.time() - process_start\n        #print(f'Batch {batch_idx} processed in {total_time:.2f}s (Load: {load_time:.2f}s, Backward: {backward_time:.2f}s, Detach: {detach_time:.2f}s, IoU: {iou_time:.2f}s, For Loop line took: {for_line_time:.2f}s)')\n        \n        #previous_loop_end_time = time.time()\n    \n    #epoch_time = time.time() - epoch_start\n    #print(f'Epoch {epoch} completed in {epoch_time:.2f}s')\n\n    average_train_mse = train_mse / len(train_loader)\n    average_train_iou = train_iou_accum / len(train_loader)\n    train_losses.append(average_train_mse)\n    train_ious.append(average_train_iou)\n\n    model.eval()\n    val_mse, val_iou_accum = 0, 0\n    with torch.no_grad():\n        for images, targets in test_loader:\n            if torch.cuda.is_available():\n                images, targets = images.cuda(), targets.cuda()\n\n            outputs = model(images)\n            confidences, bboxes = get_confidence_and_bbox(outputs)\n\n            try:\n                mask = confidences >= confidence_threshold\n                if last_confident_bbox is not None and (~mask).any():\n                    bboxes[~mask, :] = last_confident_bbox\n\n                if mask.any():\n                    last_confident_bbox = bboxes[mask][-1]\n            except IndexError as e:\n                print(f\"Error processing bboxes: {str(e)}\")\n\n            loss = criterion(outputs, targets)\n            val_mse += loss.item()\n\n            predicted_corners = convert_yolo_to_corners(bboxes, image_width, image_height)\n            target_corners = convert_yolo_to_corners(targets.squeeze(), image_width, image_height)\n            iou_scores = compute_iou_batch(predicted_corners, target_corners)\n            val_iou_accum += iou_scores.mean().item()\n\n\n    average_val_loss = val_mse / len(test_loader)\n    average_val_iou = val_iou_accum / len(test_loader)\n    val_losses.append(average_val_loss)\n    val_ious.append(average_val_iou)\n\n    print(f\"Epoch {epoch + 1}, Train Loss: {average_train_mse}, Train IoU: {average_train_iou}, Val Loss: {average_val_loss}, Val IoU: {average_val_iou}\")\n\n# Визуализация потерь и IoU\nepochs = range(1, num_epochs + 1)\nplt.figure(figsize=(12, 6))\n\nplt.subplot(1, 2, 1)\nplt.plot(epochs, train_losses, label='Training Loss')\nplt.plot(epochs, val_losses, label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs, train_ious, label='Training IoU')\nplt.plot(epochs, val_ious, label='Validation IoU')\nplt.title('Training and Validation IoU')\nplt.xlabel('Epoch')\nplt.ylabel('IoU')\nplt.legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:18:53.682045Z","iopub.execute_input":"2024-05-06T19:18:53.682429Z","iopub.status.idle":"2024-05-06T19:19:44.274862Z","shell.execute_reply.started":"2024-05-06T19:18:53.682399Z","shell.execute_reply":"2024-05-06T19:19:44.273380Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_names = [img_name for img_name in os.listdir(img_dir) if img_name.endswith('.jpg')]\nimg_names.sort(key=lambda x: (x.split('_')[0], int(x.split('_')[2]), int(x.split('_')[3].split('.')[0])))\n\n\nprint(img_names)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T18:58:20.858057Z","iopub.execute_input":"2024-05-06T18:58:20.858794Z","iopub.status.idle":"2024-05-06T18:58:21.025930Z","shell.execute_reply.started":"2024-05-06T18:58:20.858757Z","shell.execute_reply":"2024-05-06T18:58:21.024969Z"},"trusted":true},"execution_count":null,"outputs":[]}]}